{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "\n",
        "\n",
        "Text: \"*Self-pollination in the style of Ernst Haeckel*\"\n",
        "\n",
        "Result:\n",
        "\n",
        "<img src=\"https://pollinations.ai/ipfs/QmUKydjhLKP26P8qT49cvsDYXjmYAShbo2Z95JxatvzwkK\" width=\"300\" height=\"300\" />\n",
        "\n",
        "\n",
        "Based on a notebook by [Katherine Crowson](https://twitter.com/RiversHaveWings).\n",
        "\n",
        "Modified by [jbuster](https://twitter.com/jbusted1). and [thomash](https://twitter.com/pollinations_ai). \n",
        "\n",
        "---\n",
        "\n",
        "It is possible to provide multiple text prompts with weights by providing them like this: \n",
        "**```oil painting: 0.5|salvador dali: 0.3|edward much:0.6|robot friend: 1.0|text:-0.5```** *In this case a negative weight next to `text` makes the model avoid text.*\n",
        "\n",
        "\n",
        "[UPD 3.11.2021] Added Gumbel sampling method suggested by Daniel Russ on Discord\n",
        "\n",
        "[UPD 2.11.2021] Added ruDALLE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pZmCmyKM9fmv",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Text Prompt\n",
        "text_input = 'the ghost of a geisha under a weeping willow, matte painting'  #@param {type: \"string\"}\n",
        "\n",
        "# Apply a (neural) super-resolution step (from 640x480 to 1280x960)\n",
        "super_resolution = False #@param {type: \"boolean\"}\n",
        "\n",
        "# Model to use. ruDALLE was recently finetuned by a russian company called Sber and could give better outputs.\n",
        "image_model = \"imagenet\"  #@param ['imagenet', 'ruDALLE']\n",
        "\n",
        "\n",
        "output_path = \"/content/output\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VIf9Z1CTsC7R"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "#@title Upscale images/video frames\n",
        "\n",
        "black_and_white = False\n",
        "\n",
        "loaded_upscale_model = False\n",
        "\n",
        "def upscale(filepath):\n",
        "  if not super_resolution:\n",
        "    return\n",
        "  global loaded_upscale_model\n",
        "  if not loaded_upscale_model:\n",
        "    # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd /content/Real-ESRGAN\n",
        "    # Set up the environment\n",
        "    !pip install git+https://github.com/xinntao/BasicSR.git\n",
        "    !pip install facexlib\n",
        "    !pip install gfpgan\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    # Download the pre-trained model\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "    %cd -\n",
        "    loaded_upscale_model = True \n",
        "  \n",
        "  %cd /content/Real-ESRGAN\n",
        "  !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus_anime_6B.pth --input $filepath --netscale 4 --outscale 2 --half --output $output_path\n",
        "  filepath_out = filepath.replace(\".jpg\",\"_out.jpg\")\n",
        "  !mv -v $filepath_out $filepath\n",
        "  %cd -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf54A275YLpt"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSfISAhyPmyp"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $output_path\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "#%cd taming-transformers\n",
        "#!git checkout 2908a53b88478e5812d619b6ac003dbb29b069a0\n",
        "#%cd -\n",
        "!pip install torch==1.8.1 torchtext\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhhdWrSxQhwg"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "!sudo apt install aria2\n",
        "from pathlib import Path\n",
        "!sudo apt install aria2\n",
        "\n",
        "model_mapping = {\n",
        "    \"ruDALLE\": { \n",
        "        \"name\": \"vqgan_openimages_f16_8192.ckpt\",\n",
        "        \"config\": \"https://pollinations.ai/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt.yaml\",\n",
        "        \"checkpoint\": \"https://pollinations.ai/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt\",\n",
        "    },\n",
        "    \"imagenet\": {\n",
        "        \"name\": \"vqgan_imagenet_f16_16384.ckpt\",\n",
        "        \"config\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1',\n",
        "        \"checkpoint\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1',        \n",
        "    } \n",
        "}\n",
        "\n",
        "selected_vqgan = model_mapping[image_model]\n",
        "vqgan_name = selected_vqgan[\"name\"]\n",
        "\n",
        "config = selected_vqgan[\"config\"]\n",
        "checkpoint = selected_vqgan[\"checkpoint\"]\n",
        "\n",
        "if not Path(vqgan_name).exists():\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{config}' -o {vqgan_name}.yaml\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{checkpoint}'  -o {vqgan_name}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXMSuW2EQWsd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "import kornia.augmentation as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvnTBhPGT1gn"
      },
      "outputs": [],
      "source": [
        "def noise_gen(shape):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    for i in reversed(range(5)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "    return noise\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "# def replace_grad(fake, real):\n",
        "#     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "# clamp_with_grad = torch.clamp\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)\n",
        "\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "    input_normed = F.normalize(input, dim=-1)\n",
        "    target_normed = F.normalize(target, dim=-1)\n",
        "    logits = input_normed @ target_normed.T * logit_scale\n",
        "    if labels is None:\n",
        "        labels = torch.arange(len(input), device=logits.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "        if args.use_augs:\n",
        "          cutouts = augs(cutouts)\n",
        "\n",
        "        if args.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        \n",
        "\n",
        "        return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "        return diff.mean(dim=1).sqrt().mean()\n",
        "\n",
        "class GaussianBlur2d(nn.Module):\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.value = value\n",
        "        if not window:\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "        if sigma:\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "            kernel /= kernel.sum()\n",
        "        else:\n",
        "            kernel = torch.ones([1])\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        input = input.view([n * c, 1, h, w])\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "        end_pad = self.kernel.shape[0] // 2\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "        return input.view([n, c, h, w])\n",
        "\n",
        "class EMATensor(nn.Module):\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "    def __init__(self, tensor, decay):\n",
        "        super().__init__()\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\n",
        "        self.decay = decay\n",
        "        self.register_buffer('accum', torch.tensor(1.))\n",
        "        self.update()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self):\n",
        "        if not self.training:\n",
        "            raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "        self.accum *= self.decay\n",
        "        self.biased.mul_(self.decay)\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\n",
        "        self.average.copy_(self.biased)\n",
        "        self.average.div_(1 - self.accum)\n",
        "\n",
        "    def forward(self):\n",
        "        if self.training:\n",
        "            return self.tensor\n",
        "        return self.average\n",
        "\n",
        "%mkdir /content/vids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4OtaLbHBN6"
      },
      "source": [
        "# ARGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLw9p5Rzacso"
      },
      "outputs": [],
      "source": [
        "step_size = 0.1 \n",
        "if vqgan_name == 'vqgan_openimages_f16_8192.ckpt':\n",
        "  step_size=0.005\n",
        "else:\n",
        "  step_size=0.1\n",
        "args = argparse.Namespace(\n",
        "    \n",
        "    prompts=[t.strip() for t in text_input.split(\"|\")],\n",
        "    size=[640, 480], \n",
        "    init_image= None,\n",
        "    init_weight= 0.5,\n",
        "\n",
        "    # clip model settings\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config=f'{vqgan_name}.yaml',         \n",
        "    vqgan_checkpoint=vqgan_name,\n",
        "    step_size=step_size,\n",
        "    \n",
        "    # cutouts / crops\n",
        "    cutn=32,\n",
        "    cut_pow=1,\n",
        "    cut_size=224,\n",
        "\n",
        "    # display\n",
        "    display_freq=5,\n",
        "    seed=None,\n",
        "    use_augs = True,\n",
        "    noise_fac= 0.1,\n",
        "\n",
        "    record_generation=True,\n",
        "\n",
        "    # noise and other constraints\n",
        "    use_noise = None,\n",
        "    constraint_regions = False,#\n",
        "    \n",
        "    \n",
        "    # add noise to embedding\n",
        "    noise_prompt_weights = None,\n",
        "    noise_prompt_seeds = [14575],#\n",
        "\n",
        "    # mse settings\n",
        "    mse_withzeros = True,\n",
        "    mse_decay_rate = 50,\n",
        "    mse_epoches = 5,\n",
        "\n",
        "    # end itteration\n",
        "    max_itter = 600,\n",
        ")\n",
        "\n",
        "mse_decay = 0\n",
        "if args.init_weight:\n",
        "  mse_decay = args.init_weight / args.mse_epoches\n",
        "\n",
        "# <AUGMENTATIONS>\n",
        "augs = nn.Sequential(\n",
        "    \n",
        "    K.RandomHorizontalFlip(p=0.5),\n",
        "    K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "    K.RandomPerspective(0.2,p=0.4, ),\n",
        "    K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "\n",
        "    )\n",
        "\n",
        "noise = noise_gen([1, 3, args.size[0], args.size[1]])\n",
        "image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n",
        "image.save('init3.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgTa_JWi7Sn"
      },
      "source": [
        "### Actually do the run..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7EDme5RYCrt"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Using device:', device)\n",
        "print('using prompts: ', args.prompts)\n",
        "\n",
        "tv_loss = TVLoss() \n",
        "\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "mse_weight = args.init_weight\n",
        "\n",
        "cut_size = args.cut_size\n",
        "# e_dim = model.quantize.e_dim\n",
        "\n",
        "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "    e_dim = 256\n",
        "    n_toks = model.quantize.n_embed\n",
        "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "else:\n",
        "    e_dim = model.quantize.e_dim\n",
        "    n_toks = model.quantize.n_e\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "\n",
        "if args.seed is not None:\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "if args.init_image:\n",
        "    pil_image = Image.open(args.init_image).convert('RGB')\n",
        "    pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "    pil_image = TF.to_tensor(pil_image)\n",
        "    if args.use_noise:\n",
        "      pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "    z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "else:\n",
        "    \n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z = one_hot @ model.quantize.embed.weight\n",
        "    else:\n",
        "        z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "if args.mse_withzeros and not args.init_image:\n",
        "  z_orig = torch.zeros_like(z)\n",
        "else:\n",
        "  z_orig = z.clone()\n",
        "\n",
        "z.requires_grad = True\n",
        "\n",
        "opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "pMs = []\n",
        "\n",
        "if args.noise_prompt_weights and args.noise_prompt_seeds:\n",
        "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "    pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "for prompt in args.prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "def synth_gumbel(z, quantize=True, saturate=True):\n",
        "    logits = model.quantize.proj(z)\n",
        "    if quantize:\n",
        "        one_hot = F.gumbel_softmax(logits, tau=1, hard=True, dim=1)\n",
        "    else:\n",
        "        one_hot = F.one_hot(logits.argmax(1), logits.shape[1]).movedim(3, 1).to(logits.dtype)\n",
        "    z_q = torch.einsum('nchw,cd->ndhw', one_hot, model.quantize.embed.weight)\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "def synth(z, quantize=True, saturate=True):\n",
        "    out = None\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "       out = synth_gumbel(z, quantize, saturate)\n",
        "    else:\n",
        "      if args.constraint_regions:\n",
        "        z = replace_grad(z, z * z_mask)\n",
        "\n",
        "      if quantize:\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "      else:\n",
        "        z_q = z.model\n",
        "\n",
        "      out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "    if saturate:\n",
        "      progress = i / args.max_itter\n",
        "      saturation = max(0,min(1,(progress - 0.25) * 2))\n",
        "      out = transforms.functional.adjust_saturation(out, saturation)\n",
        "    \n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z, True)# False)\n",
        "\n",
        "    TF.to_pil_image(out[0].cpu()).save(f'progress.png')   \n",
        "    #display.display(display.Image('progress.png')) \n",
        "\n",
        "\n",
        "def ascend_txt(i):\n",
        "    global mse_weight\n",
        "\n",
        "    out = synth(z)\n",
        "    if args.record_generation:\n",
        "      with torch.no_grad():\n",
        "        global vid_index\n",
        "        out_a = synth(z, True)#, False)\n",
        "        if vid_index % 5 == 0:\n",
        "          filename = f'{output_path}/progress_{vid_index:05}.jpg'\n",
        "          TF.to_pil_image(out_a[0].cpu()).save(filename)\n",
        "          upscale(filename)\n",
        "        vid_index += 1\n",
        "\n",
        "    cutouts = make_cutouts(out)\n",
        "    cutouts = resample(cutouts, (perceptor.visual.input_resolution, perceptor.visual.input_resolution))\n",
        "\n",
        "\n",
        "    iii = perceptor.encode_image(normalize(cutouts)).float()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        \n",
        "        global z_orig\n",
        "        \n",
        "        result.append(F.mse_loss(z, z_orig) * mse_weight / 2)\n",
        "        # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n",
        "\n",
        "            if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n",
        "              mse_weight = mse_weight - mse_decay\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "            else:\n",
        "              mse_weight = 0\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "\n",
        "    return result\n",
        "\n",
        "vid_index = 0\n",
        "def train(i):\n",
        "    \n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt(i)\n",
        "\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    \n",
        "    loss = sum(lossAll)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "    with tqdm() as pbar:\n",
        "        while True and i != args.max_itter:\n",
        "\n",
        "            train(i)\n",
        "\n",
        "            if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n",
        "              \n",
        "              opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "            i += 1\n",
        "            pbar.update()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      },
      "source": [
        "# create video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT3hKb5gJUPq",
        "outputId": "f46af883-7177-42f9-a13d-ee22d4bfde67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00005.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00006.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00007.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00008.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00009.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00010.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00011.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00012.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00013.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00014.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00015.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00016.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00017.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00018.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00019.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00020.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00021.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00022.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00023.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00024.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00025.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00026.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00027.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00028.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00029.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00030.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00031.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00032.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00033.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00034.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00035.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00036.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00037.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00038.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00039.jpg'\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "\u001b[0;35m[image2 @ 0x564ac3686000] \u001b[0m\u001b[0;33mPattern type 'glob_sequence' is deprecated: use pattern_type 'glob' instead\n",
            "\u001b[0mInput #0, image2, from '/tmp/ffmpeg/%*.jpg':\n",
            "  Duration: 00:00:06.44, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 640x480 [SAR 1:1 DAR 4:3], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mprofile High, level 2.2\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/tmp/vid_no_audio.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 640x480 [SAR 1:1 DAR 4:3], q=-1--1, 10 fps, 10240 tbn, 10 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  161 fps= 32 q=-1.0 Lsize=    3574kB time=00:00:15.80 bitrate=1852.9kbits/s speed=3.14x    \n",
            "video:3571kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.076380%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mframe I:4     Avg QP:18.73  size: 38319\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mframe P:42    Avg QP:22.18  size: 25205\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mframe B:115   Avg QP:24.53  size: 21253\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mconsecutive B-frames:  4.3%  0.0%  3.7% 91.9%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mmb I  I16..4:  5.3% 93.8%  1.0%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mmb P  I16..4:  2.8% 60.1%  1.1%  P16..4:  5.0%  4.6%  2.5%  0.0%  0.0%    skip:23.9%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mmb B  I16..4:  1.6% 18.1%  1.7%  B16..8: 16.6% 14.1%  6.5%  direct:11.3%  skip:30.1%  L0:35.8% L1:29.2% BI:35.0%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0m8x8 transform intra:89.6% inter:92.1%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mcoded y,uvDC,uvAC intra: 84.7% 45.7% 22.5% inter: 48.3% 42.2% 7.0%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mi16 v,h,dc,p:  4% 69%  3% 25%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 45% 16%  2%  2%  3%  4%  3%  5%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 20% 12%  7%  8%  9%  9%  9% 10%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mi8c dc,h,v,p: 70% 15% 10%  5%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mWeighted P-Frames: Y:19.0% UV:14.3%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mref P L0: 35.2% 17.1% 25.9% 18.6%  3.1%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mref B L0: 79.9% 14.6%  5.5%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mref B L1: 92.7%  7.3%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mkb/s:1816.65\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/vid_no_audio.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf57.83.100\n",
            "  Duration: 00:00:16.10, start: 0.000000, bitrate: 1818 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuvj420p(pc), 640x480 [SAR 1:1 DAR 4:3], 1816 kb/s, 10 fps, 10 tbr, 10240 tbn, 20 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "Input #1, lavfi, from 'anullsrc':\n",
            "  Duration: N/A, start: 0.000000, bitrate: 705 kb/s\n",
            "    Stream #1:0: Audio: pcm_u8, 44100 Hz, stereo, u8, 705 kb/s\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (copy)\n",
            "  Stream #1:0 -> #0:1 (pcm_u8 (native) -> aac (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, mp4, to '/content/output/video.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuvj420p(pc), 640x480 [SAR 1:1 DAR 4:3], q=2-31, 1816 kb/s, 10 fps, 10 tbr, 10240 tbn, 10240 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 aac\n",
            "frame=  161 fps=0.0 q=-1.0 Lsize=    3583kB time=00:00:15.83 bitrate=1853.7kbits/s speed= 153x    \n",
            "video:3571kB audio:4kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.231886%\n",
            "\u001b[1;36m[aac @ 0x563950b49700] \u001b[0mQavg: 65536.000\n",
            "Written /content/output/video.mp4\n"
          ]
        }
      ],
      "source": [
        "out_file=output_path+\"/video.mp4\"\n",
        "\n",
        "!mkdir -p /tmp/ffmpeg\n",
        "!cp $output_path/*.jpg /tmp/ffmpeg\n",
        "last_frame=!ls -t /tmp/ffmpeg/*.jpg | head -1\n",
        "last_frame = last_frame[0]\n",
        "\n",
        "# Copy last frame to start and duplicate at end so it sticks around longer\n",
        "end_still_seconds = 4\n",
        "!cp -v $last_frame /tmp/ffmpeg/0000.jpg\n",
        "for i in range(end_still_seconds * 10):\n",
        "  pad_file = f\"/tmp/ffmpeg/zzzz_pad_{i:05}.jpg\"\n",
        "  !cp -v $last_frame $pad_file\n",
        "\n",
        "!ffmpeg  -r 10 -i /tmp/ffmpeg/%*.jpg -y -c:v libx264 /tmp/vid_no_audio.mp4\n",
        "!ffmpeg -i /tmp/vid_no_audio.mp4 -f lavfi -i anullsrc -c:v copy -c:a aac -shortest -y \"$out_file\"\n",
        "\n",
        "print(\"Written\", out_file)\n",
        "!sleep 2\n",
        "!rm -r /tmp/ffmpeg"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text-To-Image - CLIP-Guided VQGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
